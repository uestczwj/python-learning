1. 迁移学习基本概念

  迁移学习的核心问题是,找到新问题和原问题之间的相似性,才可以顺利地实现知识的迁移。

1.4 与已有概念的区别和联系
   领域自适应问题是迁移学习的研究内容之一,它侧重于解决特征空间一致、类别空间一
致,仅特征分布不一致的问题。而迁移学习也可以解决上述内容不一致的情况。

1.5 负迁移
   随着研究的深入,已经有新的研究成果在逐渐克服负迁移的影响。杨强教授团队 2015 在
数据挖掘领域顶级会议 KDD 上发表了传递迁移学习文章 Transitive transfer learning [Tan et al., 2015],
提出了传递迁移学习的思想。传统迁移学习就好比是踩着一块石头过河,传递迁移学习就好比是踩着连续的两块石头。
更进一步,杨强教授团队在 2017 年人工智能领域顶级会议 AAAI 上发表了远领域迁移学习的文章 Distant domain transfer learning [Tan et al., 2017],可以用人脸来识别飞机!

2 迁移学习的研究领域
  
大体上讲,迁移学习的分类可以按照四个准则进行:按目标域有无标签分、按学习方法分、按特征分、按离线与在线形式分。

2.1 按目标域标签分
	1. 监督迁移学习 (Supervised Transfer Learning)
	2. 半监督迁移学习 (Semi-Supervised Transfer Learning)
	3. 无监督迁移学习 (Unsupervised Transfer Learning)

	少标签或无标签的问题 (半监督和无监督迁移学习),是研究的热点和难点。这也是本手册重点关注的领域。

2.2 按学习方法分
	1. 基于样本的迁移学习方法 (Instance based Transfer Learning)
	2. 基于特征的迁移学习方法 (Feature based Transfer Learning)
	3. 基于模型的迁移学习方法 (Model based Transfer Learning)
	4. 基于关系的迁移学习方法 (Relation based Transfer Learning)

    这是一个很直观的分类方式,按照数据、特征、模型的机器学习逻辑进行区分,再加上
不属于这三者中的关系模式。

2.3 按特征分
	1. 同构迁移学习 (Homogeneous Transfer Learning)
	2. 异构迁移学习 (Heterogeneous Transfer Learning)

    这也是一种很直观的方式:如果特征语义和维度都相同,那么就是同构;反之,如果特
征完全不相同,那么就是异构。举个例子来说,不同图片的迁移,就可以认为是同构;而图
片到文本的迁移,则是异构的。

2.4 按离线与在线形式分
	1. 离线迁移学习 (Offline Transfer Learning)
	2. 在线迁移学习 (Online Transfer Learning)

    目前,绝大多数的迁移学习方法,都采用了离线方式。即,源域和目标域均是给定的,
迁移一次即可。这种方式的缺点是显而易见的:算法无法对新加入的数据进行学习,模型也
无法得到更新。与之相对的,是在线的方式。即随着数据的动态加入,迁移学习算法也可以
不断地更新。

3 迁移学习的应用

3.1 计算机视觉

3.2 文本分类

3.3 时间序列

3.4 医疗健康

4 基础知识

4.1 迁移学习的问题形式化
    迁移学习的问题形式化,是进行一切研究的前提。在迁移学习中,有两个基本的概念:
领域 (Domain) 和任务 (Task)。它们是最基础的概念。定义如下:

4.1.1 领域
4.1.2 任务
4.1.3 迁移学习

4.2 总体思路
	形式化之后,我们可以进行迁移学习的研究。迁移学习的总体思路可以概括为:开发算
法来最大限度地利用有标注的领域的知识,来辅助目标领域的知识获取和学习。

一句话总结: 相似性是核心,度量准则是重要手段。

4.3 度量准则
	https://blog.csdn.net/u012684062/article/details/73395764

4.3.1 常见的几种距离

    闵可夫斯基距离
    欧几里得距离
    曼哈顿距离
    切比雪夫距离
    马氏距离
    汉明距离
    
    编辑距离
    DTW 距离
    KL 散度

4.3.2 相似度
	余弦相似度
    皮尔逊相关系数
	杰卡德相似系数  Jaccard

4.3.3 KL 散度与 JS 距离
4.3.4 最大均值差异 MMD
4.3.5 Principal Angle
4.3.6 A-distance
4.3.7 Hilbert-Schmidt Independence Criterion
4.3.8 Wasserstein Distance

4.4 迁移学习的理论保证 *
	本部分的标题中带有 * 号,有一些难度,为可看可不看的内容。此部分最常见的形式
是当自己提出的算法需要理论证明时,可以借鉴。

5 迁移学习的基本方法

	这四种基本的方法分别是:基于样本的迁移,基于模型的迁移,基于特征的迁移,
及基于关系的迁移。

    **** 基于特征和模型的迁移方法是我们的重点。 ****

5.1 基于样本迁移
5.2 基于特征迁移
	基于特征的迁移方法 (Feature based Transfer Learning) 是指将通过特征变换的方
式互相迁移 [Liu et al., 2011, Zheng et al., 2008, Hu and Yang, 2011],来减少源域和目标
域之间的差距;或者将源域和目标域的数据特征变换到统一特征空间中 [Pan et al., 2011,
Long et al., 2014b, Duan et al., 2012],然后利用传统的机器学习方法进行分类识别。根据
特征的同构和异构性,又可以分为同构和异构迁移学习。图 15很形象地表示了两种基于特
征的迁移学习方法。

5.3 基于模型迁移
	基于模型的迁移方法 (Parameter/Model based Transfer Learning) 是指从源域和目
标域中找到他们之间共享的参数信息,以实现迁移的方法。这种迁移方式要求的假设条
件是:源域中的数据与目标域中的数据可以共享一些模型的参数。其中的代表性工作主要有 [Zhao et al., 2010, Zhao et al., 2011, Pan et al., 2008b, Pan et al., 2008a]。图 16形象地表示了基于模型的迁移学习方法的基本思想。

5.4 基于关系迁移

6 第一类方法:数据分布自适应
   ( 联合分布 & 条件分布 & 边缘分布 : https://blog.csdn.net/xiaocong1990/article/details/72027008 )


	数据分布自适应 (Distribution Adaptation) 是一类最常用的迁移学习方法。这种方法
的基本思想是,由于源域和目标域的数据概率分布不同,那么最直接的方式就是通过一些变
换,将不同的数据分布的距离拉近。

6.1 边缘分布自适应
6.1.1 基本思路
	边缘分布自适应方法 (Marginal Distribution Adaptation) 的目标是减小源域和目标域
的边缘概率分布的距离,从而完成迁移学习。从形式上来说,边缘分布自适应方法是用 P (x s )
和 P (x t ) 之间的距离来近似两个领域之间的差异。

6.1.2 核心方法
	边缘分布自适应的方法最早由香港科技大学杨强教授团队提出 [Pan et al., 2011],方法
名称为迁移成分分析 (Transfer Component Analysis)。

6.1.3 扩展
	TCA 方法是迁移学习领域一个经典的方法,之后的许多研究工作都以 TCA 为基础。
我们列举部分如下:
• ACA (Adapting Component Analysis) [Dorri and Ghodsi, 2012]: 在 TCA 中加入
HSIC
• DTMKL (Domain Transfer Multiple Kernel Learning) [Duan et al., 2012]: 在 TCA
中加入了 MK-MMD,用了新的求解方式
• TJM (Transfer Joint Matching) [Long et al., 2014b]: 在优化目标中同时进行边缘分布
自适应和源域样本选择
• DDC (Deep Domain Confusion) [Tzeng et al., 2014]: 将 MMD 度量加入了深度网络
特征层的 loss 中 (我们将会在深度迁移学习中介绍此工作)
• DAN (Deep Adaptation Network) [Long et al., 2015a]: 扩展了 DDC 的工作,将 MMD
换成了 MK-MMD,并且进行多层 loss 计算 (我们将会在深度迁移学习中介绍此工作)
• DME (Distribution Matching Embedding): 先计算变换矩阵,再进行特征映射 (与
TCA 顺序相反)
• CMD (Central Moment Matching) [Zellinger et al., 2017]: MMD 着眼于一阶,此工
作将 MMD 推广到了多阶

6.2 条件分布自适应
	条件分布自适应方法 (Conditional Distribution Adaptation) 的目标是减小源域和目标
域的条件概率分布的距离,从而完成迁移学习。从形式上来说,条件分布自适应方法是用
P (y s |x s ) 和 P (y t |x t ) 之间的距离来近似两个领域之间的差异。

	目前单独利用条件分布自适应的工作较少,这些工作主要可以在 [Saito et al., 2017]
中找到。最近,中科院计算所的 Wang 等人提出了 STL 方法 (Stratified Transfer Learn-
ing) [Wang et al., 2018]。作者提出了类内迁移 (Intra-class Transfer) 的思想。指出现有的
绝大多数方法都只是学习一个全局的特征变换 (Global Domain Shift),而忽略了类内的相
似性。类内迁移可以利用类内特征,实现更好的迁移效果。

6.3 联合分布自适应
6.3.1 基本思路
	联合分布自适应方法 (Joint Distribution Adaptation) 的目标是减小源域和目标域的联
合概率分布的距离,从而完成迁移学习。从形式上来说,联合分布自适应方法是用 P (x s ) 和
P (x t ) 之间的距离、以及 P (y s |x s ) 和 P (y t |x t ) 之间的距离来近似两个领域之间的差异。

6.3.2 核心方法
	联合分布适配的 JDA 方法 [Long et al., 2013] 首次发表于 2013 年的 ICCV(计算机视
觉领域顶会,与 CVPR 类似),它的作者是当时清华大学的博士生 (现为清华大学助理教授)
龙明盛。

6.3.3 扩展
	JDA 方法是十分经典的迁移学习方法。后续的相关工作通过在 JDA 的基础上加入额
外的损失项,使得迁移学习的效果得到了很大提升。我们在这里简要介绍一些基于 JDA 的
相关工作。
• ARTL (Adaptation Regularization) [Long et al., 2014a]: 将 JDA 嵌入一个结构风险
最小化框架中,用表示定理直接学习分类器
• VDA [Tahmoresnezhad and Hashemi, 2016]: 在 JDA 的优化目标中加入了类内距和
类间距的计算
• [Hsiao et al., 2016]: 在 JDA 的基础上加入结构不变性控制
• [Hou et al., 2015]:在 JDA 的基础上加入目标域的选择
• JGSA (Joint Geometrical and Statistical Alignment) [Zhang et al., 2017a]: 在 JDA
的基础上加入类内距、类间距、标签持久化
• JAN (Joint Adaptation Network) [Long et al., 2017]: 提出了联合分布度量 JMMD,
在深度网络中进行联合分布的优化

	特别地,在最近的研究中,来自中科院计算所的 Wang 等人 [Wang et al., 2017] 注意到
了 JDA 的不足:边缘分布自适应和条件分布自适应并不是同等重要。回到图 19表示的两种
分布的问题上来。显然,当目标域是图 19(b)所示的情况时,边缘分布应该被优先考虑;而
当目标域是图 19(c)所示的情况时,条件分布应该被优先考虑。JDA 以及后来的扩展工作均
忽视了这一问题。
	作者提出了 BDA 方法 (Balanced Distribution Adaptation) 来解决这一问题。该方法
能够根据特定的数据领域,自适应地调整分布适配过程中边缘分布和条件分布的重要性。

6.4 小结
	综合上述三种概率分布自适应方法,我们可以得出如下的结论:
1. 精度比较:BDA > JDA > TCA > 条件分布自适应。
2. 将不同的概率分布自适应方法用于神经网络,是一个发展趋势。图 23展示的结果表明,
将概率分布适配加入深度网络中,往往会取得比非深度方法更好的结果。

7 第二类方法:特征选择
	特征选择法的基本假设是:源域和目标域中均含有一部分公共的特征,在这部分公共的
特征上,源领域和目标领域的数据分布是一致的。因此,此类方法的目标就是,通过机器学
习方法,选择出这部分共享的特征,即可依据这些特征构建模型。

7.1 核心方法
	这这个领域比较经典的一个方法是发表在 2006 年的 ECML-PKDD 会议上,作者提
出了一个叫做 SCL 的方法 (Structural Correspondence Learning) [Blitzer et al., 2006]。这
个方法的目标就是我们说的,找到两个领域公共的那些特征。作者将这些公共的特征叫做
Pivot feature。找出来这些 Pivot feature,就完成了迁移学习的任务。

7.2 扩展
	SCL方法是特征选择方面的经典研究工作。基于 SCL,也出现了一些扩展工作。
• Joint feature selection and subspace learning [Gu et al., 2011]:特征选择 + 子空间学习
• TJM (Transfer Joint Matching) [Long et al., 2014b]: 在优化目标中同时进行边缘分布自适应和源域样本选择
• FSSL (Feature Selection and Structure Preservation) [Li et al., 2016]: 特征选择 + 信息不变性

7.3 小结
• 特征选择法从源域和目标域中选择提取共享的特征,建立统一模型
• 通常与分布自适应方法进行结合
• 通常采用稀疏表示 ||A|| 2,1 实现特征选择


8 第三类方法:子空间学习
	子空间学习法通常假设源域和目标域数据在变换后的子空间中会有着相似的分布。我
们按照特征变换的形式,将子空间学习法分为两种:基于统计特征变换的统计特征对齐方
法,以及基于流形变换的流形学习方法。下面我们分别介绍这两种方法的基本思路和代表
性研究成果。

8.1 统计特征对齐
	统计特征对齐方法主要将数据的统计特征进行变换对齐。对齐后的数据,可以利用传
统机器学习方法构建分类器进行学习。





